{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import load_embeddings\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manitavero\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/dev/detoxify/scripts/wandb/run-20220812_133119-26809x9x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anitavero/finetune/runs/26809x9x\" target=\"_blank\">fine-waterfall-11</a></strong> to <a href=\"https://wandb.ai/anitavero/finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "wandb.init(entity=\"anitavero\", project=\"finetune\")\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, embeddings, metadata = load_embeddings('/home/ubuntu/dev/detoxify/jigsaw-toxic-comment-classification-challenge/embeddings/embeddings_t5-large_test_This_text_is_about_{}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/home/ubuntu/dev/detoxify/jigsaw-toxic-comment-classification-challenge/data/test_labels.csv', dtype={\"id\": \"string\"})\n",
    "classes = list(labels.columns)\n",
    "classes.remove(\"id\")\n",
    "y = labels[classes].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (y != -1).any(axis=1)\n",
    "y_m = y[mask, :]\n",
    "embeddings_m = embeddings[mask, :]\n",
    "y_m.shape, embeddings_m.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_m, y_m, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22498170\n",
      "Iteration 2, loss = 0.16746005\n",
      "Iteration 3, loss = 0.15783564\n",
      "Iteration 4, loss = 0.15290194\n",
      "Iteration 5, loss = 0.14867438\n",
      "Iteration 6, loss = 0.14538247\n",
      "Iteration 7, loss = 0.14318401\n",
      "Iteration 8, loss = 0.13958453\n",
      "Iteration 9, loss = 0.13692006\n",
      "Iteration 10, loss = 0.13452032\n",
      "Iteration 11, loss = 0.13128942\n",
      "Iteration 12, loss = 0.12774620\n",
      "Iteration 13, loss = 0.12491148\n",
      "Iteration 14, loss = 0.12226801\n",
      "Iteration 15, loss = 0.11916301\n",
      "Iteration 16, loss = 0.11563826\n",
      "Iteration 17, loss = 0.11295411\n",
      "Iteration 18, loss = 0.10987703\n",
      "Iteration 19, loss = 0.10684017\n",
      "Iteration 20, loss = 0.10471133\n",
      "Iteration 21, loss = 0.10090460\n",
      "Iteration 22, loss = 0.09811528\n",
      "Iteration 23, loss = 0.09526682\n",
      "Iteration 24, loss = 0.09283641\n",
      "Iteration 25, loss = 0.08971750\n",
      "Iteration 26, loss = 0.08722871\n",
      "Iteration 27, loss = 0.08432123\n",
      "Iteration 28, loss = 0.08162082\n",
      "Iteration 29, loss = 0.07947609\n",
      "Iteration 30, loss = 0.07703359\n",
      "Iteration 31, loss = 0.07417719\n",
      "Iteration 32, loss = 0.07154175\n",
      "Iteration 33, loss = 0.06955075\n",
      "Iteration 34, loss = 0.06708641\n",
      "Iteration 35, loss = 0.06473946\n",
      "Iteration 36, loss = 0.06280279\n",
      "Iteration 37, loss = 0.06082692\n",
      "Iteration 38, loss = 0.05842802\n",
      "Iteration 39, loss = 0.05581700\n",
      "Iteration 40, loss = 0.05379503\n",
      "Iteration 41, loss = 0.05262931\n",
      "Iteration 42, loss = 0.04968825\n",
      "Iteration 43, loss = 0.04844261\n",
      "Iteration 44, loss = 0.04605008\n",
      "Iteration 45, loss = 0.04446657\n",
      "Iteration 46, loss = 0.04252889\n",
      "Iteration 47, loss = 0.04066677\n",
      "Iteration 48, loss = 0.03895739\n",
      "Iteration 49, loss = 0.03704434\n",
      "Iteration 50, loss = 0.03554526\n",
      "Iteration 51, loss = 0.03436887\n",
      "Iteration 52, loss = 0.03243517\n",
      "Iteration 53, loss = 0.03144658\n",
      "Iteration 54, loss = 0.02945000\n",
      "Iteration 55, loss = 0.02803819\n",
      "Iteration 56, loss = 0.02713072\n",
      "Iteration 57, loss = 0.02559590\n",
      "Iteration 58, loss = 0.02475428\n",
      "Iteration 59, loss = 0.02383372\n",
      "Iteration 60, loss = 0.02215399\n",
      "Iteration 61, loss = 0.02144797\n",
      "Iteration 62, loss = 0.01976810\n",
      "Iteration 63, loss = 0.01914829\n",
      "Iteration 64, loss = 0.01836480\n",
      "Iteration 65, loss = 0.01714559\n",
      "Iteration 66, loss = 0.01650573\n",
      "Iteration 67, loss = 0.01568759\n",
      "Iteration 68, loss = 0.01521258\n",
      "Iteration 69, loss = 0.01418651\n",
      "Iteration 70, loss = 0.01375480\n",
      "Iteration 71, loss = 0.01375594\n",
      "Iteration 72, loss = 0.01366071\n",
      "Iteration 73, loss = 0.01183274\n",
      "Iteration 74, loss = 0.01116062\n",
      "Iteration 75, loss = 0.01066523\n",
      "Iteration 76, loss = 0.01021907\n",
      "Iteration 77, loss = 0.01005067\n",
      "Iteration 78, loss = 0.00967561\n",
      "Iteration 79, loss = 0.00911020\n",
      "Iteration 80, loss = 0.00869092\n",
      "Iteration 81, loss = 0.00875722\n",
      "Iteration 82, loss = 0.00827744\n",
      "Iteration 83, loss = 0.00750460\n",
      "Iteration 84, loss = 0.00964748\n",
      "Iteration 85, loss = 0.00973827\n",
      "Iteration 86, loss = 0.01027738\n",
      "Iteration 87, loss = 0.00715506\n",
      "Iteration 88, loss = 0.00686533\n",
      "Iteration 89, loss = 0.00634583\n",
      "Iteration 90, loss = 0.00624953\n",
      "Iteration 91, loss = 0.00609079\n",
      "Iteration 92, loss = 0.00577623\n",
      "Iteration 93, loss = 0.00598134\n",
      "Iteration 94, loss = 0.00562935\n",
      "Iteration 95, loss = 0.00558900\n",
      "Iteration 96, loss = 0.00904400\n",
      "Iteration 97, loss = 0.00828245\n",
      "Iteration 98, loss = 0.00660088\n",
      "Iteration 99, loss = 0.00615399\n",
      "Iteration 100, loss = 0.00528118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.05714955\n",
      "Iteration 2, loss = 0.02208094\n",
      "Iteration 3, loss = 0.01918709\n",
      "Iteration 4, loss = 0.01779728\n",
      "Iteration 5, loss = 0.01676192\n",
      "Iteration 6, loss = 0.01597111\n",
      "Iteration 7, loss = 0.01530683\n",
      "Iteration 8, loss = 0.01480578\n",
      "Iteration 9, loss = 0.01421314\n",
      "Iteration 10, loss = 0.01368666\n",
      "Iteration 11, loss = 0.01308836\n",
      "Iteration 12, loss = 0.01259890\n",
      "Iteration 13, loss = 0.01213982\n",
      "Iteration 14, loss = 0.01164086\n",
      "Iteration 15, loss = 0.01126325\n",
      "Iteration 16, loss = 0.01058169\n",
      "Iteration 17, loss = 0.01012415\n",
      "Iteration 18, loss = 0.00963553\n",
      "Iteration 19, loss = 0.00911546\n",
      "Iteration 20, loss = 0.00853844\n",
      "Iteration 21, loss = 0.00824575\n",
      "Iteration 22, loss = 0.00780584\n",
      "Iteration 23, loss = 0.00731816\n",
      "Iteration 24, loss = 0.00687966\n",
      "Iteration 25, loss = 0.00649606\n",
      "Iteration 26, loss = 0.00610300\n",
      "Iteration 27, loss = 0.00587533\n",
      "Iteration 28, loss = 0.00534567\n",
      "Iteration 29, loss = 0.00503042\n",
      "Iteration 30, loss = 0.00454991\n",
      "Iteration 31, loss = 0.00423114\n",
      "Iteration 32, loss = 0.00402013\n",
      "Iteration 33, loss = 0.00364436\n",
      "Iteration 34, loss = 0.00351883\n",
      "Iteration 35, loss = 0.00317411\n",
      "Iteration 36, loss = 0.00294897\n",
      "Iteration 37, loss = 0.00270591\n",
      "Iteration 38, loss = 0.00257230\n",
      "Iteration 39, loss = 0.00243305\n",
      "Iteration 40, loss = 0.00223112\n",
      "Iteration 41, loss = 0.00211664\n",
      "Iteration 42, loss = 0.00186799\n",
      "Iteration 43, loss = 0.00183156\n",
      "Iteration 44, loss = 0.00176684\n",
      "Iteration 45, loss = 0.00165317\n",
      "Iteration 46, loss = 0.00154134\n",
      "Iteration 47, loss = 0.00142984\n",
      "Iteration 48, loss = 0.00138004\n",
      "Iteration 49, loss = 0.00136053\n",
      "Iteration 50, loss = 0.00138255\n",
      "Iteration 51, loss = 0.00129912\n",
      "Iteration 52, loss = 0.00133203\n",
      "Iteration 53, loss = 0.00123239\n",
      "Iteration 54, loss = 0.00111219\n",
      "Iteration 55, loss = 0.00108225\n",
      "Iteration 56, loss = 0.00148174\n",
      "Iteration 57, loss = 0.00224115\n",
      "Iteration 58, loss = 0.00177998\n",
      "Iteration 59, loss = 0.00106212\n",
      "Iteration 60, loss = 0.00100463\n",
      "Iteration 61, loss = 0.00102921\n",
      "Iteration 62, loss = 0.00100399\n",
      "Iteration 63, loss = 0.00103087\n",
      "Iteration 64, loss = 0.00103362\n",
      "Iteration 65, loss = 0.00116353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16917511\n",
      "Iteration 2, loss = 0.11561674\n",
      "Iteration 3, loss = 0.10650259\n",
      "Iteration 4, loss = 0.10194063\n",
      "Iteration 5, loss = 0.09822628\n",
      "Iteration 6, loss = 0.09568217\n",
      "Iteration 7, loss = 0.09336698\n",
      "Iteration 8, loss = 0.09111539\n",
      "Iteration 9, loss = 0.08871427\n",
      "Iteration 10, loss = 0.08697731\n",
      "Iteration 11, loss = 0.08478977\n",
      "Iteration 12, loss = 0.08308185\n",
      "Iteration 13, loss = 0.08120880\n",
      "Iteration 14, loss = 0.07845682\n",
      "Iteration 15, loss = 0.07688148\n",
      "Iteration 16, loss = 0.07499659\n",
      "Iteration 17, loss = 0.07282381\n",
      "Iteration 18, loss = 0.07022124\n",
      "Iteration 19, loss = 0.06878755\n",
      "Iteration 20, loss = 0.06650455\n",
      "Iteration 21, loss = 0.06525357\n",
      "Iteration 22, loss = 0.06307662\n",
      "Iteration 23, loss = 0.06070911\n",
      "Iteration 24, loss = 0.05964651\n",
      "Iteration 25, loss = 0.05706639\n",
      "Iteration 26, loss = 0.05491974\n",
      "Iteration 27, loss = 0.05400963\n",
      "Iteration 28, loss = 0.05178309\n",
      "Iteration 29, loss = 0.04975276\n",
      "Iteration 30, loss = 0.04810705\n",
      "Iteration 31, loss = 0.04592320\n",
      "Iteration 32, loss = 0.04457761\n",
      "Iteration 33, loss = 0.04270990\n",
      "Iteration 34, loss = 0.04100869\n",
      "Iteration 35, loss = 0.03923158\n",
      "Iteration 36, loss = 0.03780537\n",
      "Iteration 37, loss = 0.03607622\n",
      "Iteration 38, loss = 0.03422149\n",
      "Iteration 39, loss = 0.03303163\n",
      "Iteration 40, loss = 0.03137106\n",
      "Iteration 41, loss = 0.03101996\n",
      "Iteration 42, loss = 0.02840075\n",
      "Iteration 43, loss = 0.02706249\n",
      "Iteration 44, loss = 0.02545381\n",
      "Iteration 45, loss = 0.02495353\n",
      "Iteration 46, loss = 0.02303429\n",
      "Iteration 47, loss = 0.02171143\n",
      "Iteration 48, loss = 0.02150950\n",
      "Iteration 49, loss = 0.01936899\n",
      "Iteration 50, loss = 0.01852064\n",
      "Iteration 51, loss = 0.01759501\n",
      "Iteration 52, loss = 0.01666724\n",
      "Iteration 53, loss = 0.01578951\n",
      "Iteration 54, loss = 0.01473638\n",
      "Iteration 55, loss = 0.01413957\n",
      "Iteration 56, loss = 0.01374492\n",
      "Iteration 57, loss = 0.01249146\n",
      "Iteration 58, loss = 0.01174933\n",
      "Iteration 59, loss = 0.01113745\n",
      "Iteration 60, loss = 0.01022010\n",
      "Iteration 61, loss = 0.00959596\n",
      "Iteration 62, loss = 0.00930801\n",
      "Iteration 63, loss = 0.00895239\n",
      "Iteration 64, loss = 0.00805293\n",
      "Iteration 65, loss = 0.00762736\n",
      "Iteration 66, loss = 0.00708844\n",
      "Iteration 67, loss = 0.00684006\n",
      "Iteration 68, loss = 0.00640577\n",
      "Iteration 69, loss = 0.00619757\n",
      "Iteration 70, loss = 0.00860128\n",
      "Iteration 71, loss = 0.00795495\n",
      "Iteration 72, loss = 0.00626928\n",
      "Iteration 73, loss = 0.00574704\n",
      "Iteration 74, loss = 0.00558111\n",
      "Iteration 75, loss = 0.00485733\n",
      "Iteration 76, loss = 0.00529288\n",
      "Iteration 77, loss = 0.00510669\n",
      "Iteration 78, loss = 0.00461451\n",
      "Iteration 79, loss = 0.00478542\n",
      "Iteration 80, loss = 0.00431921\n",
      "Iteration 81, loss = 0.00393751\n",
      "Iteration 82, loss = 0.00380618\n",
      "Iteration 83, loss = 0.00370055\n",
      "Iteration 84, loss = 0.00358393\n",
      "Iteration 85, loss = 0.00623697\n",
      "Iteration 86, loss = 0.00618927\n",
      "Iteration 87, loss = 0.00500501\n",
      "Iteration 88, loss = 0.00356180\n",
      "Iteration 89, loss = 0.00333564\n",
      "Iteration 90, loss = 0.00323598\n",
      "Iteration 91, loss = 0.00320676\n",
      "Iteration 92, loss = 0.00318270\n",
      "Iteration 93, loss = 0.00316732\n",
      "Iteration 94, loss = 0.00313786\n",
      "Iteration 95, loss = 0.00324854\n",
      "Iteration 96, loss = 0.00418817\n",
      "Iteration 97, loss = 0.00330432\n",
      "Iteration 98, loss = 0.00296723\n",
      "Iteration 99, loss = 0.00290364\n",
      "Iteration 100, loss = 0.00281733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.04750812\n",
      "Iteration 2, loss = 0.01651497\n",
      "Iteration 3, loss = 0.01350375\n",
      "Iteration 4, loss = 0.01161083\n",
      "Iteration 5, loss = 0.01043928\n",
      "Iteration 6, loss = 0.00959441\n",
      "Iteration 7, loss = 0.00896649\n",
      "Iteration 8, loss = 0.00833521\n",
      "Iteration 9, loss = 0.00785745\n",
      "Iteration 10, loss = 0.00735793\n",
      "Iteration 11, loss = 0.00709400\n",
      "Iteration 12, loss = 0.00667127\n",
      "Iteration 13, loss = 0.00637211\n",
      "Iteration 14, loss = 0.00594137\n",
      "Iteration 15, loss = 0.00543760\n",
      "Iteration 16, loss = 0.00511903\n",
      "Iteration 17, loss = 0.00472784\n",
      "Iteration 18, loss = 0.00439077\n",
      "Iteration 19, loss = 0.00413484\n",
      "Iteration 20, loss = 0.00385333\n",
      "Iteration 21, loss = 0.00344824\n",
      "Iteration 22, loss = 0.00330583\n",
      "Iteration 23, loss = 0.00294729\n",
      "Iteration 24, loss = 0.00263656\n",
      "Iteration 25, loss = 0.00249522\n",
      "Iteration 26, loss = 0.00231880\n",
      "Iteration 27, loss = 0.00211768\n",
      "Iteration 28, loss = 0.00196518\n",
      "Iteration 29, loss = 0.00176852\n",
      "Iteration 30, loss = 0.00168248\n",
      "Iteration 31, loss = 0.00147736\n",
      "Iteration 32, loss = 0.00130791\n",
      "Iteration 33, loss = 0.00125594\n",
      "Iteration 34, loss = 0.00139630\n",
      "Iteration 35, loss = 0.00106426\n",
      "Iteration 36, loss = 0.00117849\n",
      "Iteration 37, loss = 0.00101777\n",
      "Iteration 38, loss = 0.00095048\n",
      "Iteration 39, loss = 0.00108685\n",
      "Iteration 40, loss = 0.00104591\n",
      "Iteration 41, loss = 0.00101108\n",
      "Iteration 42, loss = 0.00109777\n",
      "Iteration 43, loss = 0.00079020\n",
      "Iteration 44, loss = 0.00081176\n",
      "Iteration 45, loss = 0.00076116\n",
      "Iteration 46, loss = 0.00091853\n",
      "Iteration 47, loss = 0.00088221\n",
      "Iteration 48, loss = 0.00073453\n",
      "Iteration 49, loss = 0.00068355\n",
      "Iteration 50, loss = 0.00062011\n",
      "Iteration 51, loss = 0.00061343\n",
      "Iteration 52, loss = 0.00065850\n",
      "Iteration 53, loss = 0.00064464\n",
      "Iteration 54, loss = 0.00106468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16651108\n",
      "Iteration 2, loss = 0.11554429\n",
      "Iteration 3, loss = 0.10614975\n",
      "Iteration 4, loss = 0.10120468\n",
      "Iteration 5, loss = 0.09772320\n",
      "Iteration 6, loss = 0.09498208\n",
      "Iteration 7, loss = 0.09276367\n",
      "Iteration 8, loss = 0.09068714\n",
      "Iteration 9, loss = 0.08823296\n",
      "Iteration 10, loss = 0.08690657\n",
      "Iteration 11, loss = 0.08479188\n",
      "Iteration 12, loss = 0.08298547\n",
      "Iteration 13, loss = 0.08034181\n",
      "Iteration 14, loss = 0.07831713\n",
      "Iteration 15, loss = 0.07677066\n",
      "Iteration 16, loss = 0.07454290\n",
      "Iteration 17, loss = 0.07207795\n",
      "Iteration 18, loss = 0.06993024\n",
      "Iteration 19, loss = 0.06794657\n",
      "Iteration 20, loss = 0.06592656\n",
      "Iteration 21, loss = 0.06377964\n",
      "Iteration 22, loss = 0.06235141\n",
      "Iteration 23, loss = 0.05981627\n",
      "Iteration 24, loss = 0.05790346\n",
      "Iteration 25, loss = 0.05634080\n",
      "Iteration 26, loss = 0.05404323\n",
      "Iteration 27, loss = 0.05227518\n",
      "Iteration 28, loss = 0.04994531\n",
      "Iteration 29, loss = 0.04806473\n",
      "Iteration 30, loss = 0.04657061\n",
      "Iteration 31, loss = 0.04468247\n",
      "Iteration 32, loss = 0.04306456\n",
      "Iteration 33, loss = 0.04100447\n",
      "Iteration 34, loss = 0.03964146\n",
      "Iteration 35, loss = 0.03779143\n",
      "Iteration 36, loss = 0.03625574\n",
      "Iteration 37, loss = 0.03490930\n",
      "Iteration 38, loss = 0.03292160\n",
      "Iteration 39, loss = 0.03177519\n",
      "Iteration 40, loss = 0.03027985\n",
      "Iteration 41, loss = 0.03051144\n",
      "Iteration 42, loss = 0.02706225\n",
      "Iteration 43, loss = 0.02599263\n",
      "Iteration 44, loss = 0.02482055\n",
      "Iteration 45, loss = 0.02370970\n",
      "Iteration 46, loss = 0.02243264\n",
      "Iteration 47, loss = 0.02113164\n",
      "Iteration 48, loss = 0.02017274\n",
      "Iteration 49, loss = 0.01948819\n",
      "Iteration 50, loss = 0.01813274\n",
      "Iteration 51, loss = 0.01730707\n",
      "Iteration 52, loss = 0.01626892\n",
      "Iteration 53, loss = 0.01538871\n",
      "Iteration 54, loss = 0.01468282\n",
      "Iteration 55, loss = 0.01426972\n",
      "Iteration 56, loss = 0.01322674\n",
      "Iteration 57, loss = 0.01249160\n",
      "Iteration 58, loss = 0.01148899\n",
      "Iteration 59, loss = 0.01122580\n",
      "Iteration 60, loss = 0.01042996\n",
      "Iteration 61, loss = 0.01026256\n",
      "Iteration 62, loss = 0.00952777\n",
      "Iteration 63, loss = 0.00972494\n",
      "Iteration 64, loss = 0.00833298\n",
      "Iteration 65, loss = 0.00805767\n",
      "Iteration 66, loss = 0.00740724\n",
      "Iteration 67, loss = 0.00733806\n",
      "Iteration 68, loss = 0.00729443\n",
      "Iteration 69, loss = 0.00676081\n",
      "Iteration 70, loss = 0.00621111\n",
      "Iteration 71, loss = 0.00603901\n",
      "Iteration 72, loss = 0.00572917\n",
      "Iteration 73, loss = 0.00580080\n",
      "Iteration 74, loss = 0.00813407\n",
      "Iteration 75, loss = 0.00863323\n",
      "Iteration 76, loss = 0.00486806\n",
      "Iteration 77, loss = 0.00455584\n",
      "Iteration 78, loss = 0.00445293\n",
      "Iteration 79, loss = 0.00439664\n",
      "Iteration 80, loss = 0.00419753\n",
      "Iteration 81, loss = 0.00409235\n",
      "Iteration 82, loss = 0.00476359\n",
      "Iteration 83, loss = 0.00410156\n",
      "Iteration 84, loss = 0.00405192\n",
      "Iteration 85, loss = 0.00408110\n",
      "Iteration 86, loss = 0.00433840\n",
      "Iteration 87, loss = 0.00357771\n",
      "Iteration 88, loss = 0.00373897\n",
      "Iteration 89, loss = 0.00420588\n",
      "Iteration 90, loss = 0.00369908\n",
      "Iteration 91, loss = 0.00398809\n",
      "Iteration 92, loss = 0.00392682\n",
      "Iteration 93, loss = 0.00367521\n",
      "Iteration 94, loss = 0.00377313\n",
      "Iteration 95, loss = 0.00951588\n",
      "Iteration 96, loss = 0.00404276\n",
      "Iteration 97, loss = 0.00379044\n",
      "Iteration 98, loss = 0.00306586\n",
      "Iteration 99, loss = 0.00306234\n",
      "Iteration 100, loss = 0.00292040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.07650579\n",
      "Iteration 2, loss = 0.04063282\n",
      "Iteration 3, loss = 0.03550833\n",
      "Iteration 4, loss = 0.03281448\n",
      "Iteration 5, loss = 0.03066304\n",
      "Iteration 6, loss = 0.02911505\n",
      "Iteration 7, loss = 0.02767295\n",
      "Iteration 8, loss = 0.02640991\n",
      "Iteration 9, loss = 0.02522935\n",
      "Iteration 10, loss = 0.02395869\n",
      "Iteration 11, loss = 0.02297139\n",
      "Iteration 12, loss = 0.02203265\n",
      "Iteration 13, loss = 0.02112568\n",
      "Iteration 14, loss = 0.02055499\n",
      "Iteration 15, loss = 0.01969023\n",
      "Iteration 16, loss = 0.01891322\n",
      "Iteration 17, loss = 0.01854548\n",
      "Iteration 18, loss = 0.01755764\n",
      "Iteration 19, loss = 0.01689975\n",
      "Iteration 20, loss = 0.01618949\n",
      "Iteration 21, loss = 0.01553711\n",
      "Iteration 22, loss = 0.01461584\n",
      "Iteration 23, loss = 0.01378362\n",
      "Iteration 24, loss = 0.01322585\n",
      "Iteration 25, loss = 0.01271620\n",
      "Iteration 26, loss = 0.01181586\n",
      "Iteration 27, loss = 0.01144396\n",
      "Iteration 28, loss = 0.01054481\n",
      "Iteration 29, loss = 0.01017965\n",
      "Iteration 30, loss = 0.00967366\n",
      "Iteration 31, loss = 0.00912046\n",
      "Iteration 32, loss = 0.00873183\n",
      "Iteration 33, loss = 0.00792988\n",
      "Iteration 34, loss = 0.00729901\n",
      "Iteration 35, loss = 0.00674098\n",
      "Iteration 36, loss = 0.00622130\n",
      "Iteration 37, loss = 0.00599356\n",
      "Iteration 38, loss = 0.00542208\n",
      "Iteration 39, loss = 0.00526477\n",
      "Iteration 40, loss = 0.00470483\n",
      "Iteration 41, loss = 0.00434448\n",
      "Iteration 42, loss = 0.00391285\n",
      "Iteration 43, loss = 0.00377193\n",
      "Iteration 44, loss = 0.00353979\n",
      "Iteration 45, loss = 0.00322756\n",
      "Iteration 46, loss = 0.00308348\n",
      "Iteration 47, loss = 0.00283420\n",
      "Iteration 48, loss = 0.00262701\n",
      "Iteration 49, loss = 0.00231401\n",
      "Iteration 50, loss = 0.00216200\n",
      "Iteration 51, loss = 0.00212233\n",
      "Iteration 52, loss = 0.00197100\n",
      "Iteration 53, loss = 0.00183914\n",
      "Iteration 54, loss = 0.00193344\n",
      "Iteration 55, loss = 0.00166316\n",
      "Iteration 56, loss = 0.00159345\n",
      "Iteration 57, loss = 0.00151927\n",
      "Iteration 58, loss = 0.00158908\n",
      "Iteration 59, loss = 0.00143309\n",
      "Iteration 60, loss = 0.00152760\n",
      "Iteration 61, loss = 0.00155895\n",
      "Iteration 62, loss = 0.00130737\n",
      "Iteration 63, loss = 0.00126658\n",
      "Iteration 64, loss = 0.00121789\n",
      "Iteration 65, loss = 0.00119512\n",
      "Iteration 66, loss = 0.00120157\n",
      "Iteration 67, loss = 0.00123594\n",
      "Iteration 68, loss = 0.00117882\n",
      "Iteration 69, loss = 0.00112246\n",
      "Iteration 70, loss = 0.00111631\n",
      "Iteration 71, loss = 0.00111872\n",
      "Iteration 72, loss = 0.00108013\n",
      "Iteration 73, loss = 0.00109035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=1, max_iter=100, verbose=True)\n",
    "mo_clf = MultiOutputClassifier(estimator=clf).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(embs):\n",
    "    preds = mo_clf.predict_proba(embs)\n",
    "    return np.column_stack([p[:, 1] for p in preds])\n",
    "\n",
    "y_probs = predict_prob(X_test)\n",
    "y_pred = mo_clf.predict(X_test)\n",
    "score = mo_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_eval.eval_predictions import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc_scores': [0.9332486899501415,\n",
       "  0.9698520767649705,\n",
       "  0.9364603929743527,\n",
       "  0.9578011147486553,\n",
       "  0.9391207122163853,\n",
       "  0.9428209870325803],\n",
       " 'mean_auc': 0.9465506622811809}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gwfebgo4\n",
      "Sweep URL: https://wandb.ai/anitavero/uncategorized/sweeps/gwfebgo4\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "  \"name\" : \"my-sweep\",\n",
    "  \"method\" : \"bayes\",\n",
    "  \"metric\": {\n",
    "    \"name\": \"val_loss\",\n",
    "    \"goal\": \"minimize\"\n",
    "  },\n",
    "  \"parameters\" : {\n",
    "    \"epochs\" : {\n",
    "      \"values\" : [10, 20, 50]\n",
    "    },\n",
    "    \"learning_rate\" :{\n",
    "      \"min\": 0.0001,\n",
    "      \"max\": 0.1\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config = wandb.config\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        loss = mo_clf.fit(X_train, y_train)  # your model training code here\n",
    "        wandb.log({\"loss\": loss, \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2v2wkzew with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.017130624069023218\n",
      "Exception in thread Thread-31 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_11812/2152270066.py\", line 3, in train\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 128, in __getitem__\n",
      "    return self._items[key]\n",
      "KeyError: 'epochs'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 305, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3324, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 256, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 222, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1678, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1689, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 368, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pre27avw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.045715833181874735\n",
      "Exception in thread Thread-32 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_11812/2152270066.py\", line 3, in train\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 128, in __getitem__\n",
      "    return self._items[key]\n",
      "KeyError: 'epochs'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 305, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3324, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 256, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 222, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1678, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1689, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 368, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ybwn4137 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0443995968710142\n",
      "Exception in thread Thread-33 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_11812/2152270066.py\", line 3, in train\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 128, in __getitem__\n",
      "    return self._items[key]\n",
      "KeyError: 'epochs'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 305, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3324, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 256, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 222, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1678, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1689, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 368, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1jcgjf7r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05262043092598667\n",
      "Exception in thread Thread-34 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_11812/2152270066.py\", line 3, in train\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 128, in __getitem__\n",
      "    return self._items[key]\n",
      "KeyError: 'epochs'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 305, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3324, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 256, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 222, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1678, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1689, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 368, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3wa0483n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.04969723386928239\n",
      "Exception in thread Thread-35 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_11812/2152270066.py\", line 3, in train\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 128, in __getitem__\n",
      "    return self._items[key]\n",
      "KeyError: 'epochs'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 305, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3324, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 256, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 222, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1678, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1689, in _finish\n",
      "    hook.call()\n",
      "  File \"/home/ubuntu/anaconda3/envs/unitary_detoxify/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 368, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n"
     ]
    }
   ],
   "source": [
    "count = 5 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=train, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('unitary_detoxify')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d715a8e0c373335788d71af889f0d8a209e19c45567465c102df97a24f52ada6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
