{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import load_embeddings\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2vsh2utq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-sun-9</strong>: <a href=\"https://wandb.ai/anitavero/finetune/runs/2vsh2utq\" target=\"_blank\">https://wandb.ai/anitavero/finetune/runs/2vsh2utq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220812_101712-2vsh2utq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2vsh2utq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/dev/detoxify/scripts/wandb/run-20220812_101737-270dr8dm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anitavero/finetune/runs/270dr8dm\" target=\"_blank\">earnest-pond-10</a></strong> to <a href=\"https://wandb.ai/anitavero/finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "wandb.init(entity=\"anitavero\", project=\"finetune\")\n",
    "%env WANDB_SILENT=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, embeddings, metadata = load_embeddings('/home/ubuntu/dev/detoxify/jigsaw-toxic-comment-classification-challenge/embeddings/embeddings_t5-large_test_This_text_is_about_{}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/home/ubuntu/dev/detoxify/jigsaw-toxic-comment-classification-challenge/data/test_labels.csv', dtype={\"id\": \"string\"})\n",
    "classes = list(labels.columns)\n",
    "classes.remove(\"id\")\n",
    "y = labels[classes].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (y != -1).any(axis=1)\n",
    "y_m = y[mask, :]\n",
    "embeddings_m = embeddings[mask, :]\n",
    "y_m.shape, embeddings_m.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_m, y_m, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22498170\n",
      "Iteration 2, loss = 0.16746005\n",
      "Iteration 3, loss = 0.15783564\n",
      "Iteration 4, loss = 0.15290194\n",
      "Iteration 5, loss = 0.14867438\n",
      "Iteration 6, loss = 0.14538247\n",
      "Iteration 7, loss = 0.14318401\n",
      "Iteration 8, loss = 0.13958453\n",
      "Iteration 9, loss = 0.13692006\n",
      "Iteration 10, loss = 0.13452032\n",
      "Iteration 11, loss = 0.13128942\n",
      "Iteration 12, loss = 0.12774620\n",
      "Iteration 13, loss = 0.12491148\n",
      "Iteration 14, loss = 0.12226801\n",
      "Iteration 15, loss = 0.11916301\n",
      "Iteration 16, loss = 0.11563826\n",
      "Iteration 17, loss = 0.11295411\n",
      "Iteration 18, loss = 0.10987703\n",
      "Iteration 19, loss = 0.10684017\n",
      "Iteration 20, loss = 0.10471133\n",
      "Iteration 21, loss = 0.10090460\n",
      "Iteration 22, loss = 0.09811528\n",
      "Iteration 23, loss = 0.09526682\n",
      "Iteration 24, loss = 0.09283641\n",
      "Iteration 25, loss = 0.08971750\n",
      "Iteration 26, loss = 0.08722871\n",
      "Iteration 27, loss = 0.08432123\n",
      "Iteration 28, loss = 0.08162082\n",
      "Iteration 29, loss = 0.07947609\n",
      "Iteration 30, loss = 0.07703359\n",
      "Iteration 31, loss = 0.07417719\n",
      "Iteration 32, loss = 0.07154175\n",
      "Iteration 33, loss = 0.06955075\n",
      "Iteration 34, loss = 0.06708641\n",
      "Iteration 35, loss = 0.06473946\n",
      "Iteration 36, loss = 0.06280279\n",
      "Iteration 37, loss = 0.06082692\n",
      "Iteration 38, loss = 0.05842802\n",
      "Iteration 39, loss = 0.05581700\n",
      "Iteration 40, loss = 0.05379503\n",
      "Iteration 41, loss = 0.05262931\n",
      "Iteration 42, loss = 0.04968825\n",
      "Iteration 43, loss = 0.04844261\n",
      "Iteration 44, loss = 0.04605008\n",
      "Iteration 45, loss = 0.04446657\n",
      "Iteration 46, loss = 0.04252889\n",
      "Iteration 47, loss = 0.04066677\n",
      "Iteration 48, loss = 0.03895739\n",
      "Iteration 49, loss = 0.03704434\n",
      "Iteration 50, loss = 0.03554526\n",
      "Iteration 51, loss = 0.03436887\n",
      "Iteration 52, loss = 0.03243517\n",
      "Iteration 53, loss = 0.03144658\n",
      "Iteration 54, loss = 0.02945000\n",
      "Iteration 55, loss = 0.02803819\n",
      "Iteration 56, loss = 0.02713072\n",
      "Iteration 57, loss = 0.02559590\n",
      "Iteration 58, loss = 0.02475428\n",
      "Iteration 59, loss = 0.02383372\n",
      "Iteration 60, loss = 0.02215399\n",
      "Iteration 61, loss = 0.02144797\n",
      "Iteration 62, loss = 0.01976810\n",
      "Iteration 63, loss = 0.01914829\n",
      "Iteration 64, loss = 0.01836480\n",
      "Iteration 65, loss = 0.01714559\n",
      "Iteration 66, loss = 0.01650573\n",
      "Iteration 67, loss = 0.01568759\n",
      "Iteration 68, loss = 0.01521258\n",
      "Iteration 69, loss = 0.01418651\n",
      "Iteration 70, loss = 0.01375480\n",
      "Iteration 71, loss = 0.01375594\n",
      "Iteration 72, loss = 0.01366071\n",
      "Iteration 73, loss = 0.01183274\n",
      "Iteration 74, loss = 0.01116062\n",
      "Iteration 75, loss = 0.01066523\n",
      "Iteration 76, loss = 0.01021907\n",
      "Iteration 77, loss = 0.01005067\n",
      "Iteration 78, loss = 0.00967561\n",
      "Iteration 79, loss = 0.00911020\n",
      "Iteration 80, loss = 0.00869092\n",
      "Iteration 81, loss = 0.00875722\n",
      "Iteration 82, loss = 0.00827744\n",
      "Iteration 83, loss = 0.00750460\n",
      "Iteration 84, loss = 0.00964748\n",
      "Iteration 85, loss = 0.00973827\n",
      "Iteration 86, loss = 0.01027738\n",
      "Iteration 87, loss = 0.00715506\n",
      "Iteration 88, loss = 0.00686533\n",
      "Iteration 89, loss = 0.00634583\n",
      "Iteration 90, loss = 0.00624953\n",
      "Iteration 91, loss = 0.00609079\n",
      "Iteration 92, loss = 0.00577623\n",
      "Iteration 93, loss = 0.00598134\n",
      "Iteration 94, loss = 0.00562935\n",
      "Iteration 95, loss = 0.00558900\n",
      "Iteration 96, loss = 0.00904400\n",
      "Iteration 97, loss = 0.00828245\n",
      "Iteration 98, loss = 0.00660088\n",
      "Iteration 99, loss = 0.00615399\n",
      "Iteration 100, loss = 0.00528118\n",
      "Iteration 101, loss = 0.00492579\n",
      "Iteration 102, loss = 0.00510439\n",
      "Iteration 103, loss = 0.00487390\n",
      "Iteration 104, loss = 0.00509854\n",
      "Iteration 105, loss = 0.00474844\n",
      "Iteration 106, loss = 0.00500130\n",
      "Iteration 107, loss = 0.00472537\n",
      "Iteration 108, loss = 0.00451540\n",
      "Iteration 109, loss = 0.00867864\n",
      "Iteration 110, loss = 0.00847969\n",
      "Iteration 111, loss = 0.00483148\n",
      "Iteration 112, loss = 0.00484834\n",
      "Iteration 113, loss = 0.00481987\n",
      "Iteration 114, loss = 0.00471293\n",
      "Iteration 115, loss = 0.00466511\n",
      "Iteration 116, loss = 0.00434385\n",
      "Iteration 117, loss = 0.00684100\n",
      "Iteration 118, loss = 0.00595026\n",
      "Iteration 119, loss = 0.00412959\n",
      "Iteration 120, loss = 0.00428597\n",
      "Iteration 121, loss = 0.00450702\n",
      "Iteration 122, loss = 0.00435040\n",
      "Iteration 123, loss = 0.00421178\n",
      "Iteration 124, loss = 0.00399123\n",
      "Iteration 125, loss = 0.00957640\n",
      "Iteration 126, loss = 0.00558593\n",
      "Iteration 127, loss = 0.00481227\n",
      "Iteration 128, loss = 0.00458749\n",
      "Iteration 129, loss = 0.00391069\n",
      "Iteration 130, loss = 0.00437612\n",
      "Iteration 131, loss = 0.00400663\n",
      "Iteration 132, loss = 0.00434404\n",
      "Iteration 133, loss = 0.00418555\n",
      "Iteration 134, loss = 0.00897874\n",
      "Iteration 135, loss = 0.00621617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05714955\n",
      "Iteration 2, loss = 0.02208094\n",
      "Iteration 3, loss = 0.01918709\n",
      "Iteration 4, loss = 0.01779728\n",
      "Iteration 5, loss = 0.01676192\n",
      "Iteration 6, loss = 0.01597111\n",
      "Iteration 7, loss = 0.01530683\n",
      "Iteration 8, loss = 0.01480578\n",
      "Iteration 9, loss = 0.01421314\n",
      "Iteration 10, loss = 0.01368666\n",
      "Iteration 11, loss = 0.01308836\n",
      "Iteration 12, loss = 0.01259890\n",
      "Iteration 13, loss = 0.01213982\n",
      "Iteration 14, loss = 0.01164086\n",
      "Iteration 15, loss = 0.01126325\n",
      "Iteration 16, loss = 0.01058169\n",
      "Iteration 17, loss = 0.01012415\n",
      "Iteration 18, loss = 0.00963553\n",
      "Iteration 19, loss = 0.00911546\n",
      "Iteration 20, loss = 0.00853844\n",
      "Iteration 21, loss = 0.00824575\n",
      "Iteration 22, loss = 0.00780584\n",
      "Iteration 23, loss = 0.00731816\n",
      "Iteration 24, loss = 0.00687966\n",
      "Iteration 25, loss = 0.00649606\n",
      "Iteration 26, loss = 0.00610300\n",
      "Iteration 27, loss = 0.00587533\n",
      "Iteration 28, loss = 0.00534567\n",
      "Iteration 29, loss = 0.00503042\n",
      "Iteration 30, loss = 0.00454991\n",
      "Iteration 31, loss = 0.00423114\n",
      "Iteration 32, loss = 0.00402013\n",
      "Iteration 33, loss = 0.00364436\n",
      "Iteration 34, loss = 0.00351883\n",
      "Iteration 35, loss = 0.00317411\n",
      "Iteration 36, loss = 0.00294897\n",
      "Iteration 37, loss = 0.00270591\n",
      "Iteration 38, loss = 0.00257230\n",
      "Iteration 39, loss = 0.00243305\n",
      "Iteration 40, loss = 0.00223112\n",
      "Iteration 41, loss = 0.00211664\n",
      "Iteration 42, loss = 0.00186799\n",
      "Iteration 43, loss = 0.00183156\n",
      "Iteration 44, loss = 0.00176684\n",
      "Iteration 45, loss = 0.00165317\n",
      "Iteration 46, loss = 0.00154134\n",
      "Iteration 47, loss = 0.00142984\n",
      "Iteration 48, loss = 0.00138004\n",
      "Iteration 49, loss = 0.00136053\n",
      "Iteration 50, loss = 0.00138255\n",
      "Iteration 51, loss = 0.00129912\n",
      "Iteration 52, loss = 0.00133203\n",
      "Iteration 53, loss = 0.00123239\n",
      "Iteration 54, loss = 0.00111219\n",
      "Iteration 55, loss = 0.00108225\n",
      "Iteration 56, loss = 0.00148174\n",
      "Iteration 57, loss = 0.00224115\n",
      "Iteration 58, loss = 0.00177998\n",
      "Iteration 59, loss = 0.00106212\n",
      "Iteration 60, loss = 0.00100463\n",
      "Iteration 61, loss = 0.00102921\n",
      "Iteration 62, loss = 0.00100399\n",
      "Iteration 63, loss = 0.00103087\n",
      "Iteration 64, loss = 0.00103362\n",
      "Iteration 65, loss = 0.00116353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16917511\n",
      "Iteration 2, loss = 0.11561674\n",
      "Iteration 3, loss = 0.10650259\n",
      "Iteration 4, loss = 0.10194063\n",
      "Iteration 5, loss = 0.09822628\n",
      "Iteration 6, loss = 0.09568217\n",
      "Iteration 7, loss = 0.09336698\n",
      "Iteration 8, loss = 0.09111539\n",
      "Iteration 9, loss = 0.08871427\n",
      "Iteration 10, loss = 0.08697731\n",
      "Iteration 11, loss = 0.08478977\n",
      "Iteration 12, loss = 0.08308185\n",
      "Iteration 13, loss = 0.08120880\n",
      "Iteration 14, loss = 0.07845682\n",
      "Iteration 15, loss = 0.07688148\n",
      "Iteration 16, loss = 0.07499659\n",
      "Iteration 17, loss = 0.07282381\n",
      "Iteration 18, loss = 0.07022124\n",
      "Iteration 19, loss = 0.06878755\n",
      "Iteration 20, loss = 0.06650455\n",
      "Iteration 21, loss = 0.06525357\n",
      "Iteration 22, loss = 0.06307662\n",
      "Iteration 23, loss = 0.06070911\n",
      "Iteration 24, loss = 0.05964651\n",
      "Iteration 25, loss = 0.05706639\n",
      "Iteration 26, loss = 0.05491974\n",
      "Iteration 27, loss = 0.05400963\n",
      "Iteration 28, loss = 0.05178309\n",
      "Iteration 29, loss = 0.04975276\n",
      "Iteration 30, loss = 0.04810705\n",
      "Iteration 31, loss = 0.04592320\n",
      "Iteration 32, loss = 0.04457761\n",
      "Iteration 33, loss = 0.04270990\n",
      "Iteration 34, loss = 0.04100869\n",
      "Iteration 35, loss = 0.03923158\n",
      "Iteration 36, loss = 0.03780537\n",
      "Iteration 37, loss = 0.03607622\n",
      "Iteration 38, loss = 0.03422149\n",
      "Iteration 39, loss = 0.03303163\n",
      "Iteration 40, loss = 0.03137106\n",
      "Iteration 41, loss = 0.03101996\n",
      "Iteration 42, loss = 0.02840075\n",
      "Iteration 43, loss = 0.02706249\n",
      "Iteration 44, loss = 0.02545381\n",
      "Iteration 45, loss = 0.02495353\n",
      "Iteration 46, loss = 0.02303429\n",
      "Iteration 47, loss = 0.02171143\n",
      "Iteration 48, loss = 0.02150950\n",
      "Iteration 49, loss = 0.01936899\n",
      "Iteration 50, loss = 0.01852064\n",
      "Iteration 51, loss = 0.01759501\n",
      "Iteration 52, loss = 0.01666724\n",
      "Iteration 53, loss = 0.01578951\n",
      "Iteration 54, loss = 0.01473638\n",
      "Iteration 55, loss = 0.01413957\n",
      "Iteration 56, loss = 0.01374492\n",
      "Iteration 57, loss = 0.01249146\n",
      "Iteration 58, loss = 0.01174933\n",
      "Iteration 59, loss = 0.01113745\n",
      "Iteration 60, loss = 0.01022010\n",
      "Iteration 61, loss = 0.00959596\n",
      "Iteration 62, loss = 0.00930801\n",
      "Iteration 63, loss = 0.00895239\n",
      "Iteration 64, loss = 0.00805293\n",
      "Iteration 65, loss = 0.00762736\n",
      "Iteration 66, loss = 0.00708844\n",
      "Iteration 67, loss = 0.00684006\n",
      "Iteration 68, loss = 0.00640577\n",
      "Iteration 69, loss = 0.00619757\n",
      "Iteration 70, loss = 0.00860128\n",
      "Iteration 71, loss = 0.00795495\n",
      "Iteration 72, loss = 0.00626928\n",
      "Iteration 73, loss = 0.00574704\n",
      "Iteration 74, loss = 0.00558111\n",
      "Iteration 75, loss = 0.00485733\n",
      "Iteration 76, loss = 0.00529288\n",
      "Iteration 77, loss = 0.00510669\n",
      "Iteration 78, loss = 0.00461451\n",
      "Iteration 79, loss = 0.00478542\n",
      "Iteration 80, loss = 0.00431921\n",
      "Iteration 81, loss = 0.00393751\n",
      "Iteration 82, loss = 0.00380618\n",
      "Iteration 83, loss = 0.00370055\n",
      "Iteration 84, loss = 0.00358393\n",
      "Iteration 85, loss = 0.00623697\n",
      "Iteration 86, loss = 0.00618927\n",
      "Iteration 87, loss = 0.00500501\n",
      "Iteration 88, loss = 0.00356180\n",
      "Iteration 89, loss = 0.00333564\n",
      "Iteration 90, loss = 0.00323598\n",
      "Iteration 91, loss = 0.00320676\n",
      "Iteration 92, loss = 0.00318270\n",
      "Iteration 93, loss = 0.00316732\n",
      "Iteration 94, loss = 0.00313786\n",
      "Iteration 95, loss = 0.00324854\n",
      "Iteration 96, loss = 0.00418817\n",
      "Iteration 97, loss = 0.00330432\n",
      "Iteration 98, loss = 0.00296723\n",
      "Iteration 99, loss = 0.00290364\n",
      "Iteration 100, loss = 0.00281733\n",
      "Iteration 101, loss = 0.00280822\n",
      "Iteration 102, loss = 0.00273111\n",
      "Iteration 103, loss = 0.00810741\n",
      "Iteration 104, loss = 0.00575724\n",
      "Iteration 105, loss = 0.00404690\n",
      "Iteration 106, loss = 0.00311806\n",
      "Iteration 107, loss = 0.00328556\n",
      "Iteration 108, loss = 0.00275321\n",
      "Iteration 109, loss = 0.00272221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04750812\n",
      "Iteration 2, loss = 0.01651497\n",
      "Iteration 3, loss = 0.01350375\n",
      "Iteration 4, loss = 0.01161083\n",
      "Iteration 5, loss = 0.01043928\n",
      "Iteration 6, loss = 0.00959441\n",
      "Iteration 7, loss = 0.00896649\n",
      "Iteration 8, loss = 0.00833521\n",
      "Iteration 9, loss = 0.00785745\n",
      "Iteration 10, loss = 0.00735793\n",
      "Iteration 11, loss = 0.00709400\n",
      "Iteration 12, loss = 0.00667127\n",
      "Iteration 13, loss = 0.00637211\n",
      "Iteration 14, loss = 0.00594137\n",
      "Iteration 15, loss = 0.00543760\n",
      "Iteration 16, loss = 0.00511903\n",
      "Iteration 17, loss = 0.00472784\n",
      "Iteration 18, loss = 0.00439077\n",
      "Iteration 19, loss = 0.00413484\n",
      "Iteration 20, loss = 0.00385333\n",
      "Iteration 21, loss = 0.00344824\n",
      "Iteration 22, loss = 0.00330583\n",
      "Iteration 23, loss = 0.00294729\n",
      "Iteration 24, loss = 0.00263656\n",
      "Iteration 25, loss = 0.00249522\n",
      "Iteration 26, loss = 0.00231880\n",
      "Iteration 27, loss = 0.00211768\n",
      "Iteration 28, loss = 0.00196518\n",
      "Iteration 29, loss = 0.00176852\n",
      "Iteration 30, loss = 0.00168248\n",
      "Iteration 31, loss = 0.00147736\n",
      "Iteration 32, loss = 0.00130791\n",
      "Iteration 33, loss = 0.00125594\n",
      "Iteration 34, loss = 0.00139630\n",
      "Iteration 35, loss = 0.00106426\n",
      "Iteration 36, loss = 0.00117849\n",
      "Iteration 37, loss = 0.00101777\n",
      "Iteration 38, loss = 0.00095048\n",
      "Iteration 39, loss = 0.00108685\n",
      "Iteration 40, loss = 0.00104591\n",
      "Iteration 41, loss = 0.00101108\n",
      "Iteration 42, loss = 0.00109777\n",
      "Iteration 43, loss = 0.00079020\n",
      "Iteration 44, loss = 0.00081176\n",
      "Iteration 45, loss = 0.00076116\n",
      "Iteration 46, loss = 0.00091853\n",
      "Iteration 47, loss = 0.00088221\n",
      "Iteration 48, loss = 0.00073453\n",
      "Iteration 49, loss = 0.00068355\n",
      "Iteration 50, loss = 0.00062011\n",
      "Iteration 51, loss = 0.00061343\n",
      "Iteration 52, loss = 0.00065850\n",
      "Iteration 53, loss = 0.00064464\n",
      "Iteration 54, loss = 0.00106468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16651108\n",
      "Iteration 2, loss = 0.11554429\n",
      "Iteration 3, loss = 0.10614975\n",
      "Iteration 4, loss = 0.10120468\n",
      "Iteration 5, loss = 0.09772320\n",
      "Iteration 6, loss = 0.09498208\n",
      "Iteration 7, loss = 0.09276367\n",
      "Iteration 8, loss = 0.09068714\n",
      "Iteration 9, loss = 0.08823296\n",
      "Iteration 10, loss = 0.08690657\n",
      "Iteration 11, loss = 0.08479188\n",
      "Iteration 12, loss = 0.08298547\n",
      "Iteration 13, loss = 0.08034181\n",
      "Iteration 14, loss = 0.07831713\n",
      "Iteration 15, loss = 0.07677066\n",
      "Iteration 16, loss = 0.07454290\n",
      "Iteration 17, loss = 0.07207795\n",
      "Iteration 18, loss = 0.06993024\n",
      "Iteration 19, loss = 0.06794657\n",
      "Iteration 20, loss = 0.06592656\n",
      "Iteration 21, loss = 0.06377964\n",
      "Iteration 22, loss = 0.06235141\n",
      "Iteration 23, loss = 0.05981627\n",
      "Iteration 24, loss = 0.05790346\n",
      "Iteration 25, loss = 0.05634080\n",
      "Iteration 26, loss = 0.05404323\n",
      "Iteration 27, loss = 0.05227518\n",
      "Iteration 28, loss = 0.04994531\n",
      "Iteration 29, loss = 0.04806473\n",
      "Iteration 30, loss = 0.04657061\n",
      "Iteration 31, loss = 0.04468247\n",
      "Iteration 32, loss = 0.04306456\n",
      "Iteration 33, loss = 0.04100447\n",
      "Iteration 34, loss = 0.03964146\n",
      "Iteration 35, loss = 0.03779143\n",
      "Iteration 36, loss = 0.03625574\n",
      "Iteration 37, loss = 0.03490930\n",
      "Iteration 38, loss = 0.03292160\n",
      "Iteration 39, loss = 0.03177519\n",
      "Iteration 40, loss = 0.03027985\n",
      "Iteration 41, loss = 0.03051144\n",
      "Iteration 42, loss = 0.02706225\n",
      "Iteration 43, loss = 0.02599263\n",
      "Iteration 44, loss = 0.02482055\n",
      "Iteration 45, loss = 0.02370970\n",
      "Iteration 46, loss = 0.02243264\n",
      "Iteration 47, loss = 0.02113164\n",
      "Iteration 48, loss = 0.02017274\n",
      "Iteration 49, loss = 0.01948819\n",
      "Iteration 50, loss = 0.01813274\n",
      "Iteration 51, loss = 0.01730707\n",
      "Iteration 52, loss = 0.01626892\n",
      "Iteration 53, loss = 0.01538871\n",
      "Iteration 54, loss = 0.01468282\n",
      "Iteration 55, loss = 0.01426972\n",
      "Iteration 56, loss = 0.01322674\n",
      "Iteration 57, loss = 0.01249160\n",
      "Iteration 58, loss = 0.01148899\n",
      "Iteration 59, loss = 0.01122580\n",
      "Iteration 60, loss = 0.01042996\n",
      "Iteration 61, loss = 0.01026256\n",
      "Iteration 62, loss = 0.00952777\n",
      "Iteration 63, loss = 0.00972494\n",
      "Iteration 64, loss = 0.00833298\n",
      "Iteration 65, loss = 0.00805767\n",
      "Iteration 66, loss = 0.00740724\n",
      "Iteration 67, loss = 0.00733806\n",
      "Iteration 68, loss = 0.00729443\n",
      "Iteration 69, loss = 0.00676081\n",
      "Iteration 70, loss = 0.00621111\n",
      "Iteration 71, loss = 0.00603901\n",
      "Iteration 72, loss = 0.00572917\n",
      "Iteration 73, loss = 0.00580080\n",
      "Iteration 74, loss = 0.00813407\n",
      "Iteration 75, loss = 0.00863323\n",
      "Iteration 76, loss = 0.00486806\n",
      "Iteration 77, loss = 0.00455584\n",
      "Iteration 78, loss = 0.00445293\n",
      "Iteration 79, loss = 0.00439664\n",
      "Iteration 80, loss = 0.00419753\n",
      "Iteration 81, loss = 0.00409235\n",
      "Iteration 82, loss = 0.00476359\n",
      "Iteration 83, loss = 0.00410156\n",
      "Iteration 84, loss = 0.00405192\n",
      "Iteration 85, loss = 0.00408110\n",
      "Iteration 86, loss = 0.00433840\n",
      "Iteration 87, loss = 0.00357771\n",
      "Iteration 88, loss = 0.00373897\n",
      "Iteration 89, loss = 0.00420588\n",
      "Iteration 90, loss = 0.00369908\n",
      "Iteration 91, loss = 0.00398809\n",
      "Iteration 92, loss = 0.00392682\n",
      "Iteration 93, loss = 0.00367521\n",
      "Iteration 94, loss = 0.00377313\n",
      "Iteration 95, loss = 0.00951588\n",
      "Iteration 96, loss = 0.00404276\n",
      "Iteration 97, loss = 0.00379044\n",
      "Iteration 98, loss = 0.00306586\n",
      "Iteration 99, loss = 0.00306234\n",
      "Iteration 100, loss = 0.00292040\n",
      "Iteration 101, loss = 0.00299104\n",
      "Iteration 102, loss = 0.00297795\n",
      "Iteration 103, loss = 0.00305621\n",
      "Iteration 104, loss = 0.00285931\n",
      "Iteration 105, loss = 0.00290416\n",
      "Iteration 106, loss = 0.00307127\n",
      "Iteration 107, loss = 0.00279203\n",
      "Iteration 108, loss = 0.00309008\n",
      "Iteration 109, loss = 0.00333698\n",
      "Iteration 110, loss = 0.00325077\n",
      "Iteration 111, loss = 0.00468311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.07650579\n",
      "Iteration 2, loss = 0.04063282\n",
      "Iteration 3, loss = 0.03550833\n",
      "Iteration 4, loss = 0.03281448\n",
      "Iteration 5, loss = 0.03066304\n",
      "Iteration 6, loss = 0.02911505\n",
      "Iteration 7, loss = 0.02767295\n",
      "Iteration 8, loss = 0.02640991\n",
      "Iteration 9, loss = 0.02522935\n",
      "Iteration 10, loss = 0.02395869\n",
      "Iteration 11, loss = 0.02297139\n",
      "Iteration 12, loss = 0.02203265\n",
      "Iteration 13, loss = 0.02112568\n",
      "Iteration 14, loss = 0.02055499\n",
      "Iteration 15, loss = 0.01969023\n",
      "Iteration 16, loss = 0.01891322\n",
      "Iteration 17, loss = 0.01854548\n",
      "Iteration 18, loss = 0.01755764\n",
      "Iteration 19, loss = 0.01689975\n",
      "Iteration 20, loss = 0.01618949\n",
      "Iteration 21, loss = 0.01553711\n",
      "Iteration 22, loss = 0.01461584\n",
      "Iteration 23, loss = 0.01378362\n",
      "Iteration 24, loss = 0.01322585\n",
      "Iteration 25, loss = 0.01271620\n",
      "Iteration 26, loss = 0.01181586\n",
      "Iteration 27, loss = 0.01144396\n",
      "Iteration 28, loss = 0.01054481\n",
      "Iteration 29, loss = 0.01017965\n",
      "Iteration 30, loss = 0.00967366\n",
      "Iteration 31, loss = 0.00912046\n",
      "Iteration 32, loss = 0.00873183\n",
      "Iteration 33, loss = 0.00792988\n",
      "Iteration 34, loss = 0.00729901\n",
      "Iteration 35, loss = 0.00674098\n",
      "Iteration 36, loss = 0.00622130\n",
      "Iteration 37, loss = 0.00599356\n",
      "Iteration 38, loss = 0.00542208\n",
      "Iteration 39, loss = 0.00526477\n",
      "Iteration 40, loss = 0.00470483\n",
      "Iteration 41, loss = 0.00434448\n",
      "Iteration 42, loss = 0.00391285\n",
      "Iteration 43, loss = 0.00377193\n",
      "Iteration 44, loss = 0.00353979\n",
      "Iteration 45, loss = 0.00322756\n",
      "Iteration 46, loss = 0.00308348\n",
      "Iteration 47, loss = 0.00283420\n",
      "Iteration 48, loss = 0.00262701\n",
      "Iteration 49, loss = 0.00231401\n",
      "Iteration 50, loss = 0.00216200\n",
      "Iteration 51, loss = 0.00212233\n",
      "Iteration 52, loss = 0.00197100\n",
      "Iteration 53, loss = 0.00183914\n",
      "Iteration 54, loss = 0.00193344\n",
      "Iteration 55, loss = 0.00166316\n",
      "Iteration 56, loss = 0.00159345\n",
      "Iteration 57, loss = 0.00151927\n",
      "Iteration 58, loss = 0.00158908\n",
      "Iteration 59, loss = 0.00143309\n",
      "Iteration 60, loss = 0.00152760\n",
      "Iteration 61, loss = 0.00155895\n",
      "Iteration 62, loss = 0.00130737\n",
      "Iteration 63, loss = 0.00126658\n",
      "Iteration 64, loss = 0.00121789\n",
      "Iteration 65, loss = 0.00119512\n",
      "Iteration 66, loss = 0.00120157\n",
      "Iteration 67, loss = 0.00123594\n",
      "Iteration 68, loss = 0.00117882\n",
      "Iteration 69, loss = 0.00112246\n",
      "Iteration 70, loss = 0.00111631\n",
      "Iteration 71, loss = 0.00111872\n",
      "Iteration 72, loss = 0.00108013\n",
      "Iteration 73, loss = 0.00109035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=1, max_iter=300, verbose=True)\n",
    "mo_clf = MultiOutputClassifier(estimator=clf).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(embs):\n",
    "    preds = mo_clf.predict_proba(embs)\n",
    "    return np.column_stack([p[:, 1] for p in preds])\n",
    "\n",
    "y_probs = predict_prob(X_test)\n",
    "y_pred = mo_clf.predict(X_test)\n",
    "score = mo_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_eval.eval_predictions import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc_scores': [0.9313865491070665,\n",
       "  0.9698520767649705,\n",
       "  0.9377424028792546,\n",
       "  0.9578011147486553,\n",
       "  0.9373657230578611,\n",
       "  0.9428209870325803],\n",
       " 'mean_auc': 0.946161475598398}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(y_probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('unitary_detoxify')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d715a8e0c373335788d71af889f0d8a209e19c45567465c102df97a24f52ada6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
